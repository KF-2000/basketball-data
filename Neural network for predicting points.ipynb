{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasRegressor"
   ]
  },
  {
   "source": [
    "The first think being done is this notebook is collecting the feature data and label data for my model which in this case is 2-point field goals made 2P, 3-point field goals made 3P, free throws made FT and the points scored PTS. For this model I will be applying weights to each of my features to proportionately represent each goal type's contribution to the total points, since each type of goal is worth a different amount of points. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Basketball-Reference doesn't have a specific 2-point field goals made column for each gamelog, so to find this I can find the difference between total field goals FG and 3-point field goals 3P, where the left over numbers are 2-point field goals 2P."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The names of each player I've collected data for and which seasons I've collected for each player in order.\n",
    "\n",
    "name = ['James Harden', 'Anthony Davis', 'LeBron James', 'Giannis Antetokounmpo', 'Kevin Durant', 'Russell Westbrook', 'Victor Oladipo', 'Paul George', 'Joel Embiid', 'Devin Booker', 'Bradley Beal', 'Trae Young', 'Luka Doncic']\n",
    "season = [['2018','2019','2020'],['2018','2020'],['2018'],['2018','2019','2020'],['2018','2019'],['2018'],['2018'],['2019'],['2019'],['2019'],['2019'],['2020'],['2020']]\n",
    "\n",
    "# First I'm going to load all of the feature data into a single dataframe.\n",
    "\n",
    "feature_training = pd.DataFrame()\n",
    "feature_testing = pd.DataFrame()\n",
    "label_training = pd.DataFrame()\n",
    "label_testing = pd.DataFrame()\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(name)):\n",
    "    for j in range(len(season[i])):\n",
    "\n",
    "        # Downloading the file data from the appropriate file path.\n",
    "        # Loading a player's feature and label data for one season.\n",
    "\n",
    "        feature_data = pd.read_csv(r'C:\\Users\\frank\\OneDrive\\Documents\\DS\\ML Basketball Data\\Player Data\\{}\\{}'.format(name[i],season[i][j]),index_col=0)\n",
    "        label_data = pd.read_csv(r'C:\\Users\\frank\\OneDrive\\Documents\\DS\\ML Basketball Data\\Player Data\\{}\\Points\\{}'.format(name[i],season[i][j]),index_col=0)\n",
    "\n",
    "        feature_data['2P'] = feature_data['FG'] - feature_data['3P'] # finding the 2P fields goals for each row.\n",
    "\n",
    "        # Splitting the data for each csv into train/test data\n",
    "\n",
    "        feature_train, feature_test, label_train, label_test = train_test_split(feature_data,label_data,test_size=0.2,random_state=5) # using the same random state will ensure the same indices are used for the train/test split on each set of data.\n",
    "\n",
    "        # After splitting the data I will load it into different train and test dataframes for both the features and label data.\n",
    "        \n",
    "        feature_training = pd.concat([feature_training,feature_train])\n",
    "        feature_testing = pd.concat([feature_testing,feature_test])\n",
    "        label_training = pd.concat([label_training,label_train])\n",
    "        label_testing = pd.concat([label_testing,label_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only selecting the most significant features for our model.\n",
    "\n",
    "features = ['2P','3P','FT']\n",
    "feature_training = feature_training[features]\n",
    "feature_testing = feature_testing[features]\n",
    "\n",
    "feature_training.reset_index(drop='True', inplace=True)\n",
    "label_training.reset_index(drop='True', inplace=True)\n",
    "feature_testing.reset_index(drop='True', inplace=True)\n",
    "label_testing.reset_index(drop='True', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "True\nTrue\nFalse\nFalse\n"
     ]
    }
   ],
   "source": [
    "# Checking for nan values (which our network cannot process)\n",
    "print(feature_training.isnull().values.any())\n",
    "print(label_training.isnull().values.any())\n",
    "print(feature_testing.isnull().values.any())\n",
    "print(label_testing.isnull().values.any())"
   ]
  },
  {
   "source": [
    "So we have NaN values in all of our training data which the network won't be able to process."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_f = [] # list to contain the indices of rows with nan values in the feature training data\n",
    "for i in range(len(feature_training)):\n",
    "    if str(feature_training.iloc[i].isnull().values.any()) == 'True':\n",
    "        null_f.append(i)\n",
    "\n",
    "null_l = [] # list to contain the indices of rows with nan values in the label training data\n",
    "for i in range(len(label_training)):\n",
    "    if str(label_training.iloc[i].isnull().values.any()) == 'True':\n",
    "        null_l.append(i)"
   ]
  },
  {
   "source": [
    "So the rows that contain null values are the same in both the feature and label training data so we can now remove them."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_training = feature_training.drop(null_f,axis=0)\n",
    "label_training = label_training.drop(null_l,axis=0)"
   ]
  },
  {
   "source": [
    "Keras also can't read Pandas Dataframes so, now I'll convert my dataframe into Numpy arrays."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_training = np.array(feature_training)\n",
    "label_training = np.array(label_training)\n",
    "feature_testing = np.array(feature_testing)\n",
    "label_testing = np.array(label_testing)"
   ]
  },
  {
   "source": [
    "Now to create the neural network I'll be using to generate the model."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.keras.backend.clear_session()\n",
    "# Creating a fully connected neural network with 3 inputs and 4 hidden layers\n",
    "\n",
    "def wide_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(256,input_dim=feature_training.shape[1],activation='relu'))\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error']) # using mean squared error as my loss function and RMSprop as my optimizer\n",
    "\n",
    "    return model\n",
    "\n",
    "def deep_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16,input_dim=feature_training.shape[1],activation='relu'))\n",
    "    model.add(Dense(16,activation='relu'))\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(25,activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error']) # using mean squared error as my loss function and RMSprop as my optimizer\n",
    "\n",
    "    return model\n",
    "\n",
    "def deep_wide_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32,input_dim=feature_training.shape[1],activation='relu'))\n",
    "    model.add(Dense(32,activation='relu'))\n",
    "    model.add(Dense(64,activation='relu'))\n",
    "    model.add(Dense(128,activation='relu'))\n",
    "    model.add(Dense(256,activation='relu'))\n",
    "    model.add(Dense(87,activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mean_squared_error']) # using mean squared error as my loss function and RMSprop as my optimizer\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The MSE for training with a wide neural network is -0.019620121505194973\n",
      "The accuracy for predictions with a wide neural network is 0.978494623655914\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler() # scaler to normalise the dataset\n",
    "\n",
    "wide = [] \n",
    "wide.append(('standardize', scaler))\n",
    "wide.append(('mlp', KerasRegressor(build_fn=wide_model, epochs=50, batch_size=5, verbose=0))) # putting the pipeline process which will standardise the data and create the model\n",
    "pipeline = Pipeline(wide)\n",
    "\n",
    "stratkf = StratifiedKFold(n_splits=5,shuffle=True) # using 5 splits since I only have 1077 training samples and 279 testing samples\n",
    "results = cross_val_score(pipeline, feature_training, label_training, cv=stratkf, scoring='neg_mean_squared_error') # finding the mean squared error for each split of the data\n",
    "print('The MSE for training with a wide neural network is',results.mean()) # the average mean squared error for each valuation model\n",
    "\n",
    "predictions = cross_val_predict(pipeline, feature_testing, label_testing) # generating predictions\n",
    "accuracy = accuracy_score(np.round(label_testing).flatten(), np.round(predictions)) # accuracy of predictions\n",
    "print('The accuracy for predictions with a wide neural network is', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The MSE for training with a deep neural network is -1.2359708570091965\n",
      "The accuracy for predictions with a deep neural network is 0.5663082437275986\n"
     ]
    }
   ],
   "source": [
    "deep = [] \n",
    "deep.append(('standardize', scaler))\n",
    "deep.append(('mlp', KerasRegressor(build_fn=deep_model, epochs=50, batch_size=5, verbose=0))) # putting the pipeline process which will standardise the data and create the model\n",
    "pipeline = Pipeline(deep)\n",
    "\n",
    "stratkf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "results = cross_val_score(pipeline, feature_training, label_training, cv=stratkf, scoring='neg_mean_squared_error') # finding the mean squared error for each split of the data\n",
    "print('The MSE for training with a deep neural network is',results.mean()) # the average mean squared error for each valuation model\n",
    "\n",
    "predictions = cross_val_predict(pipeline, feature_testing, label_testing) # generating predictions\n",
    "accuracy = accuracy_score(np.round(label_testing).flatten(), np.round(predictions)) # accuracy of predictions\n",
    "print('The accuracy for predictions with a deep neural network is', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The MSE for training with a neural network thats both deep and widening -0.1056580626963582\n",
      "The accuracy for predictions with a neural network thats both deep and widening 0.6953405017921147\n"
     ]
    }
   ],
   "source": [
    "deep_wide = [] \n",
    "deep_wide.append(('standardize', scaler))\n",
    "deep_wide.append(('mlp', KerasRegressor(build_fn=deep_wide_model, epochs=50, batch_size=5, verbose=0))) # putting the pipeline process which will standardise the data and create the model\n",
    "pipeline = Pipeline(deep_wide)\n",
    "\n",
    "stratkf = StratifiedKFold(n_splits=5,shuffle=True)\n",
    "results = cross_val_score(pipeline, feature_training, label_training, cv=stratkf, scoring='neg_mean_squared_error') # finding the mean squared error for each split of the data\n",
    "print('The MSE for training with a neural network thats both deep and widening',results.mean()) # the average mean squared error for each valuation model\n",
    "\n",
    "predictions = cross_val_predict(pipeline, feature_testing, label_testing) # generating predictions\n",
    "accuracy = accuracy_score(np.round(label_testing).flatten(), np.round(predictions)) # accuracy of predictions\n",
    "print('The accuracy for predictions with a neural network thats both deep and widening', accuracy)"
   ]
  },
  {
   "source": [
    "For the above evaluations I created three neural networks; a wide network, a deep network and a network that increases the number of neurons in each layer. I tried to keep the number of parameters in each network roughly the same, so that the number of weights and connections were kept approximately constant, while changing the architecture of the network.\n",
    "Surprisingly I've found that the wide network is the best network"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}